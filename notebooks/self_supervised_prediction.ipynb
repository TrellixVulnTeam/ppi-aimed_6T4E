{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SageMaker self supervised prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sagemaker\n",
    "import boto3\n",
    "\n",
    "sagemaker_session = sagemaker.Session()\n",
    "account_id =  boto3.client('sts').get_caller_identity().get('Account')\n",
    "region = boto3.session.Session().region_name\n",
    "\n",
    "\n",
    "#role = sagemaker.get_execution_role()\n",
    "role=\"arn:aws:iam::{}:role/service-role/AmazonSageMaker-ExecutionRole-20190118T115449\".format(account_id)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "version_tag=\"202209190430\"\n",
    "pytorch_custom_image_name=f\"large-scale-ptm-ppi:gpu-{version_tag}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "docker_repo = \"{}.dkr.ecr.{}.amazonaws.com/{}\".format(account_id, region, pytorch_custom_image_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "bucket = \"aegovan-data\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "abstract_trainfile = \"s3://{}/self-supervised/train.json\".format(bucket)\n",
    "abstract_testfile= \"s3://{}/self-supervised/test.json\".format(bucket)\n",
    "abstract_valfile=\"s3://{}/self-supervised/val.json\".format(bucket)\n",
    "\n",
    "abstract_trainfile = \"s3://{}/self-supervised-fake/3048_767_573/train.json\".format(bucket)\n",
    "abstract_testfile= \"s3://{}/self-supervised-fake/3048_767_573/test.json\".format(bucket)\n",
    "abstract_valfile=\"s3://{}/self-supervised-fake/3048_767_573/val.json\".format(bucket)\n",
    "abstract_fake = \"s3://{}/self-supervised-fake/3048_767_573/\".format(bucket)\n",
    "\n",
    "abstract_largescale = \"s3://{}/selfsupervisedlargescale/pubmedabstracts/\".format(bucket)\n",
    "\n",
    "eval_file = abstract_fake\n",
    "filepattern =  \"{}/*.json\" # \"{}/*.tsv\" # #  # \n",
    "\n",
    "instance_type =  \"ml.g4dn.2xlarge\" # \"ml.p3.2xlarge\"  #  #ml.g4dn.2xlarge\n",
    "instance_count = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "date_fmt = datetime.datetime.today().strftime(\"%Y%m%d%H\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_job=\"selfsupervised-fake-3048-767-573-bert-f-2022-10-16-05-09-43-840\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "s3_model_path = f\"s3://aegovan-data/selfsupervised_results/{training_job}/output/model.tar.gz\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run  prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#s3_output_predictions = \"s3://aegovan-data/pubmed_asbtract/predictions_largescale_{}_{}/\".format(job_prefix,date_fmt)\n",
    "s3_output_predictions = \"s3://aegovan-data/selfsupervised_chemprot/predictions_{}/{}\".format(training_job,date_fmt)\n",
    "s3_input_data = eval_file\n",
    "s3_data_type=\"S3Prefix\"\n",
    "usefilter=0\n",
    "filter_threshold_std=1.0\n",
    "\n",
    "s3_input_models = s3_model_path\n",
    "s3_input_vocab = \"s3://{}/embeddings/bert/\".format(bucket)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('s3://aegovan-data/self-supervised-fake/3048_767_573/', 'S3Prefix')"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s3_input_data, s3_data_type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Job Name:  selfsupervised-inference-2022-10-17-02-01-31-261\n",
      "Inputs:  [{'InputName': 'input-1', 'AppManaged': False, 'S3Input': {'S3Uri': 's3://aegovan-data/self-supervised-fake/3048_767_573/', 'LocalPath': '/opt/ml/processing/input/data/jsondata', 'S3DataType': 'S3Prefix', 'S3InputMode': 'File', 'S3DataDistributionType': 'ShardedByS3Key', 'S3CompressionType': 'None'}}, {'InputName': 'input-2', 'AppManaged': False, 'S3Input': {'S3Uri': 's3://aegovan-data/selfsupervised_results/selfsupervised-fake-3048-767-573-bert-f-2022-10-16-05-09-43-840/output/model.tar.gz', 'LocalPath': '/opt/ml/processing/input/data/models', 'S3DataType': 'S3Prefix', 'S3InputMode': 'File', 'S3DataDistributionType': 'FullyReplicated', 'S3CompressionType': 'None'}}, {'InputName': 'input-3', 'AppManaged': False, 'S3Input': {'S3Uri': 's3://aegovan-data/embeddings/bert/', 'LocalPath': '/opt/ml/processing/input/data/vocab', 'S3DataType': 'S3Prefix', 'S3InputMode': 'File', 'S3DataDistributionType': 'FullyReplicated', 'S3CompressionType': 'None'}}, {'InputName': 'code', 'AppManaged': False, 'S3Input': {'S3Uri': 's3://sagemaker-us-east-2-324346001917/selfsupervised-inference-2022-10-17-02-01-31-261/input/code/chemprot_selfsupervised_batch_predict.py', 'LocalPath': '/opt/ml/processing/input/code', 'S3DataType': 'S3Prefix', 'S3InputMode': 'File', 'S3DataDistributionType': 'FullyReplicated', 'S3CompressionType': 'None'}}]\n",
      "Outputs:  [{'OutputName': 'predictions', 'AppManaged': False, 'S3Output': {'S3Uri': 's3://aegovan-data/selfsupervised_chemprot/predictions_selfsupervised-fake-3048-767-573-bert-f-2022-10-16-05-09-43-840/2022101619', 'LocalPath': '/opt/ml/processing/output', 'S3UploadMode': 'EndOfJob'}}]\n",
      "........................\u001B[34m{'datadir': '/opt/ml/processing/input/data/jsondata', 'artefactsdir': '/opt/ml/processing/input/data/models', 'outdir': '/opt/ml/processing/output', 'log_level': 'INFO', 'numworkers': None, 'batch': 32, 'ensemble': 0, 'filter': 0, 'filterstdthreshold': 1.0, 'filepattern': '{}/*.json'}\u001B[0m\n",
      "\u001B[34m{'tokenisor_data_dir': '/opt/ml/processing/input/data/vocab'}\u001B[0m\n",
      "\u001B[34m2022-10-17 02:05:34,937 - __main__ - INFO - Checking if just one tar file exists in /opt/ml/processing/input/data/models\u001B[0m\n",
      "\u001B[34m2022-10-17 02:05:38,269 - __main__ - INFO - Setting base dir to /opt/ml/processing/input/data/models/model\u001B[0m\n",
      "\u001B[34m2022-10-17 02:05:38,269 - inference.batch_predict - INFO - Running inference on file /opt/ml/processing/input/data/jsondata/val.json with output in /opt/ml/processing/output/val.json.json\u001B[0m\n",
      "\u001B[34m2022-10-17 02:05:38,269 - inference.batch_predict - INFO - Processing data file /opt/ml/processing/input/data/jsondata/val.json\u001B[0m\n",
      "\u001B[34m2022-10-17 02:05:38,269 - inference.batch_predict - INFO - Using args :{'datasetfactory': 'datasets.chemprot_selfsupervised_dataset_factory.ChemprotSelfsupervisedDatasetFactory', 'traindir': '/opt/ml/input/data/train', 'valdir': '/opt/ml/input/data/val', 'testdir': None, 'modelfactory': 'models.bert_model_factory.BertModelFactory', 'pretrained_model_dir': '/opt/ml/input/data/PRETRAINED_MODEL', 'kfoldtrainprefix': None, 'outdir': '/opt/ml/output/data', 'modeldir': '/opt/ml/model', 'checkpointdir': '/opt/ml/checkpoints/', 'checkpointfreq': '2', 'gradientaccumulationsteps': 8, 'learningrate': 1e-05, 'batch': 8, 'epochs': 200, 'earlystoppingpatience': 50, 'numworkers': None, 'uselosseval': 0, 'log_level': 'INFO', 'commit_id': 'a6211b46f5940b9ac48fd3bde9274734ec3605a5', 'tokenisor_lower_case': '0', 'weight_decay': '0.01', 'pretrained_model': '/opt/ml/input/data/PRETRAINED_MODEL', 'tokenisor_data_dir': '/opt/ml/processing/input/data/vocab'}\u001B[0m\n",
      "\u001B[34m2022-10-17 02:05:38,850 - models.bert_model_factory - INFO - Retrieving model\u001B[0m\n",
      "\u001B[34m2022-10-17 02:05:41,192 - models.bert_model_factory - INFO - Retrieving key model_fine_tune with default 0, found 0\u001B[0m\n",
      "\u001B[34m2022-10-17 02:05:41,192 - models.bert_model_factory - INFO - Retrieving model complete\u001B[0m\n",
      "\u001B[34m2022-10-17 02:05:41,193 - dataset_builder - INFO - Retrieving Tokeniser\u001B[0m\n",
      "\u001B[34m2022-10-17 02:05:41,193 - models.bert_model_factory - INFO - Retrieving Tokeniser\u001B[0m\n",
      "\u001B[34m2022-10-17 02:05:41,193 - models.bert_model_factory - INFO - Retrieving key tokenisor_max_seq_len with default 512, found 512\u001B[0m\n",
      "\u001B[34m2022-10-17 02:05:41,193 - models.bert_model_factory - INFO - Retrieving key tokenisor_lower_case with default 0, found 0\u001B[0m\n",
      "\u001B[34m2022-10-17 02:05:41,193 - models.bert_model_factory - INFO - Retrieving key pretrained_model with default bert-base-cased, found /opt/ml/input/data/PRETRAINED_MODEL\u001B[0m\n",
      "\u001B[34m2022-10-17 02:05:41,193 - models.bert_model_factory - INFO - Retrieving key tokenisor_data_dir with default /opt/ml/input/data/PRETRAINED_MODEL, found /opt/ml/processing/input/data/vocab\u001B[0m\n",
      "\u001B[34m2022-10-17 02:05:41,272 - datasets.chemprot_selfsupervised_dataset - INFO - File extension .json\u001B[0m\n",
      "\u001B[34m2022-10-17 02:05:41,298 - numexpr.utils - INFO - NumExpr defaulting to 8 threads.\u001B[0m\n",
      "\u001B[34m2022-10-17 02:05:43,215 - inference.ensemble_predictor - INFO - Using devices ['cuda:0']\u001B[0m\n",
      "\u001B[34m2022-10-17 02:05:43,344 - inference.predictor - INFO - Using device cuda:0\u001B[0m\n",
      "\u001B[34m2022-10-17 02:06:43,938 - inference.predictor - INFO - Completed inference cuda:0\u001B[0m\n",
      "\u001B[34m2022-10-17 02:06:43,938 - inference.ensemble_predictor - INFO - Computing average \u001B[0m\n",
      "\u001B[34m2022-10-17 02:06:43,939 - inference.ensemble_predictor - INFO - Computing ensemble prediction \u001B[0m\n",
      "\u001B[34m2022-10-17 02:06:43,943 - inference.ensemble_predictor - INFO - Completed ensemble prediction \u001B[0m\n",
      "\u001B[34m/opt/conda/lib/python3.6/site-packages/pandas/core/frame.py:1490: FutureWarning: Using short name for 'orient' is deprecated. Only the options: ('dict', list, 'series', 'split', 'records', 'index') will be used in a future version. Use one of the above to silence this warning.\n",
      "  FutureWarning,\u001B[0m\n",
      "\u001B[34m2022-10-17 02:06:44,009 - inference.batch_predict - INFO - Records to write: 1752\u001B[0m\n",
      "\u001B[34m2022-10-17 02:06:44,009 - inference.batch_predict - INFO - Writing to file /opt/ml/processing/output/val.json.json\u001B[0m\n",
      "\u001B[34m2022-10-17 02:06:44,058 - inference.batch_predict - INFO - Completed file /opt/ml/processing/input/data/jsondata/val.json\u001B[0m\n",
      "\u001B[34m2022-10-17 02:06:44,058 - inference.batch_predict - INFO - Running inference on file /opt/ml/processing/input/data/jsondata/train.json with output in /opt/ml/processing/output/train.json.json\u001B[0m\n",
      "\u001B[34m2022-10-17 02:06:44,058 - inference.batch_predict - INFO - Processing data file /opt/ml/processing/input/data/jsondata/train.json\u001B[0m\n",
      "\u001B[34m2022-10-17 02:06:44,058 - inference.batch_predict - INFO - Using args :{'datasetfactory': 'datasets.chemprot_selfsupervised_dataset_factory.ChemprotSelfsupervisedDatasetFactory', 'traindir': '/opt/ml/input/data/train', 'valdir': '/opt/ml/input/data/val', 'testdir': None, 'modelfactory': 'models.bert_model_factory.BertModelFactory', 'pretrained_model_dir': '/opt/ml/input/data/PRETRAINED_MODEL', 'kfoldtrainprefix': None, 'outdir': '/opt/ml/output/data', 'modeldir': '/opt/ml/model', 'checkpointdir': '/opt/ml/checkpoints/', 'checkpointfreq': '2', 'gradientaccumulationsteps': 8, 'learningrate': 1e-05, 'batch': 8, 'epochs': 200, 'earlystoppingpatience': 50, 'numworkers': None, 'uselosseval': 0, 'log_level': 'INFO', 'commit_id': 'a6211b46f5940b9ac48fd3bde9274734ec3605a5', 'tokenisor_lower_case': '0', 'weight_decay': '0.01', 'pretrained_model': '/opt/ml/input/data/PRETRAINED_MODEL', 'tokenisor_data_dir': '/opt/ml/processing/input/data/vocab'}\u001B[0m\n",
      "\u001B[34m2022-10-17 02:06:44,059 - models.bert_model_factory - INFO - Retrieving model\u001B[0m\n",
      "\u001B[34m2022-10-17 02:06:46,386 - models.bert_model_factory - INFO - Retrieving key model_fine_tune with default 0, found 0\u001B[0m\n",
      "\u001B[34m2022-10-17 02:06:46,386 - models.bert_model_factory - INFO - Retrieving model complete\u001B[0m\n",
      "\u001B[34m2022-10-17 02:06:46,386 - dataset_builder - INFO - Retrieving Tokeniser\u001B[0m\n",
      "\u001B[34m2022-10-17 02:06:46,386 - models.bert_model_factory - INFO - Retrieving Tokeniser\u001B[0m\n",
      "\u001B[34m2022-10-17 02:06:46,387 - models.bert_model_factory - INFO - Retrieving key tokenisor_max_seq_len with default 512, found 512\u001B[0m\n",
      "\u001B[34m2022-10-17 02:06:46,387 - models.bert_model_factory - INFO - Retrieving key tokenisor_lower_case with default 0, found 0\u001B[0m\n",
      "\u001B[34m2022-10-17 02:06:46,387 - models.bert_model_factory - INFO - Retrieving key pretrained_model with default bert-base-cased, found /opt/ml/input/data/PRETRAINED_MODEL\u001B[0m\n",
      "\u001B[34m2022-10-17 02:06:46,387 - models.bert_model_factory - INFO - Retrieving key tokenisor_data_dir with default /opt/ml/input/data/PRETRAINED_MODEL, found /opt/ml/processing/input/data/vocab\u001B[0m\n",
      "\u001B[34m2022-10-17 02:06:46,414 - datasets.chemprot_selfsupervised_dataset - INFO - File extension .json\u001B[0m\n",
      "\u001B[34m2022-10-17 02:06:46,456 - inference.ensemble_predictor - INFO - Using devices ['cuda:0']\u001B[0m\n",
      "\u001B[34m2022-10-17 02:06:46,570 - inference.predictor - INFO - Using device cuda:0\u001B[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001B[34m2022-10-17 02:08:47,722 - inference.predictor - INFO - Completed inference cuda:0\u001B[0m\n",
      "\u001B[34m2022-10-17 02:08:47,722 - inference.ensemble_predictor - INFO - Computing average \u001B[0m\n",
      "\u001B[34m2022-10-17 02:08:47,723 - inference.ensemble_predictor - INFO - Computing ensemble prediction \u001B[0m\n",
      "\u001B[34m2022-10-17 02:08:47,730 - inference.ensemble_predictor - INFO - Completed ensemble prediction \u001B[0m\n",
      "\u001B[34m/opt/conda/lib/python3.6/site-packages/pandas/core/frame.py:1490: FutureWarning: Using short name for 'orient' is deprecated. Only the options: ('dict', list, 'series', 'split', 'records', 'index') will be used in a future version. Use one of the above to silence this warning.\n",
      "  FutureWarning,\u001B[0m\n",
      "\u001B[34m2022-10-17 02:08:47,831 - inference.batch_predict - INFO - Records to write: 3048\u001B[0m\n",
      "\u001B[34m2022-10-17 02:08:47,831 - inference.batch_predict - INFO - Writing to file /opt/ml/processing/output/train.json.json\u001B[0m\n",
      "\u001B[34m2022-10-17 02:08:47,916 - inference.batch_predict - INFO - Completed file /opt/ml/processing/input/data/jsondata/train.json\u001B[0m\n",
      "\u001B[34m2022-10-17 02:08:47,922 - inference.batch_predict - INFO - Running inference on file /opt/ml/processing/input/data/jsondata/test.json with output in /opt/ml/processing/output/test.json.json\u001B[0m\n",
      "\u001B[34m2022-10-17 02:08:47,922 - inference.batch_predict - INFO - Processing data file /opt/ml/processing/input/data/jsondata/test.json\u001B[0m\n",
      "\u001B[34m2022-10-17 02:08:47,922 - inference.batch_predict - INFO - Using args :{'datasetfactory': 'datasets.chemprot_selfsupervised_dataset_factory.ChemprotSelfsupervisedDatasetFactory', 'traindir': '/opt/ml/input/data/train', 'valdir': '/opt/ml/input/data/val', 'testdir': None, 'modelfactory': 'models.bert_model_factory.BertModelFactory', 'pretrained_model_dir': '/opt/ml/input/data/PRETRAINED_MODEL', 'kfoldtrainprefix': None, 'outdir': '/opt/ml/output/data', 'modeldir': '/opt/ml/model', 'checkpointdir': '/opt/ml/checkpoints/', 'checkpointfreq': '2', 'gradientaccumulationsteps': 8, 'learningrate': 1e-05, 'batch': 8, 'epochs': 200, 'earlystoppingpatience': 50, 'numworkers': None, 'uselosseval': 0, 'log_level': 'INFO', 'commit_id': 'a6211b46f5940b9ac48fd3bde9274734ec3605a5', 'tokenisor_lower_case': '0', 'weight_decay': '0.01', 'pretrained_model': '/opt/ml/input/data/PRETRAINED_MODEL', 'tokenisor_data_dir': '/opt/ml/processing/input/data/vocab'}\u001B[0m\n",
      "\u001B[34m2022-10-17 02:08:47,923 - models.bert_model_factory - INFO - Retrieving model\u001B[0m\n",
      "\u001B[34m2022-10-17 02:08:50,161 - models.bert_model_factory - INFO - Retrieving key model_fine_tune with default 0, found 0\u001B[0m\n",
      "\u001B[34m2022-10-17 02:08:50,161 - models.bert_model_factory - INFO - Retrieving model complete\u001B[0m\n",
      "\u001B[34m2022-10-17 02:08:50,161 - dataset_builder - INFO - Retrieving Tokeniser\u001B[0m\n",
      "\u001B[34m2022-10-17 02:08:50,161 - models.bert_model_factory - INFO - Retrieving Tokeniser\u001B[0m\n",
      "\u001B[34m2022-10-17 02:08:50,161 - models.bert_model_factory - INFO - Retrieving key tokenisor_max_seq_len with default 512, found 512\u001B[0m\n",
      "\u001B[34m2022-10-17 02:08:50,161 - models.bert_model_factory - INFO - Retrieving key tokenisor_lower_case with default 0, found 0\u001B[0m\n",
      "\u001B[34m2022-10-17 02:08:50,161 - models.bert_model_factory - INFO - Retrieving key pretrained_model with default bert-base-cased, found /opt/ml/input/data/PRETRAINED_MODEL\u001B[0m\n",
      "\u001B[34m2022-10-17 02:08:50,161 - models.bert_model_factory - INFO - Retrieving key tokenisor_data_dir with default /opt/ml/input/data/PRETRAINED_MODEL, found /opt/ml/processing/input/data/vocab\u001B[0m\n",
      "\u001B[34m2022-10-17 02:08:50,187 - datasets.chemprot_selfsupervised_dataset - INFO - File extension .json\u001B[0m\n",
      "\u001B[34m2022-10-17 02:08:50,222 - inference.ensemble_predictor - INFO - Using devices ['cuda:0']\u001B[0m\n",
      "\u001B[34m2022-10-17 02:08:50,331 - inference.predictor - INFO - Using device cuda:0\u001B[0m\n",
      "\n",
      "\u001B[34m2022-10-17 02:10:30,563 - inference.predictor - INFO - Completed inference cuda:0\u001B[0m\n",
      "\u001B[34m2022-10-17 02:10:30,563 - inference.ensemble_predictor - INFO - Computing average \u001B[0m\n",
      "\u001B[34m2022-10-17 02:10:30,564 - inference.ensemble_predictor - INFO - Computing ensemble prediction \u001B[0m\n",
      "\u001B[34m2022-10-17 02:10:30,570 - inference.ensemble_predictor - INFO - Completed ensemble prediction \u001B[0m\n",
      "\u001B[34m/opt/conda/lib/python3.6/site-packages/pandas/core/frame.py:1490: FutureWarning: Using short name for 'orient' is deprecated. Only the options: ('dict', list, 'series', 'split', 'records', 'index') will be used in a future version. Use one of the above to silence this warning.\n",
      "  FutureWarning,\u001B[0m\n",
      "\u001B[34m2022-10-17 02:10:30,651 - inference.batch_predict - INFO - Records to write: 2461\u001B[0m\n",
      "\u001B[34m2022-10-17 02:10:30,651 - inference.batch_predict - INFO - Writing to file /opt/ml/processing/output/test.json.json\u001B[0m\n",
      "\u001B[34m2022-10-17 02:10:30,720 - inference.batch_predict - INFO - Completed file /opt/ml/processing/input/data/jsondata/test.json\u001B[0m\n"
     ]
    }
   ],
   "source": [
    "from sagemaker.network import NetworkConfig\n",
    "from sagemaker.processing import ProcessingInput, ProcessingOutput\n",
    "from sagemaker.processing import ScriptProcessor\n",
    "\n",
    "script_processor = ScriptProcessor(image_uri=docker_repo,\n",
    "                                       command=[\"python\"],\n",
    "                                       env={'mode': 'python', 'PYTHONPATH':'/opt/ml/code'},\n",
    "                                       role=role,\n",
    "                                       instance_type=instance_type,\n",
    "                                       instance_count=instance_count,\n",
    "                                       max_runtime_in_seconds= 5 * 24 * 60 * 60,\n",
    "                                       volume_size_in_gb = 200,\n",
    "                                       network_config=NetworkConfig(enable_network_isolation=False),\n",
    "                                       base_job_name =\"selfsupervised-inference\"\n",
    "                                       )\n",
    "\n",
    "\n",
    "sm_local_input_models = \"/opt/ml/processing/input/data/models\"\n",
    "sm_local_input_data = \"/opt/ml/processing/input/data/jsondata\"\n",
    "sm_local_input_vocab = \"/opt/ml/processing/input/data/vocab\"\n",
    "\n",
    "\n",
    "sm_local_output = \"/opt/ml/processing/output\"\n",
    "\n",
    "input_file_name = s3_input_data.split(\"/\")[-1]\n",
    "\n",
    "script_processor.run(\n",
    "        code='../src/inference/chemprot_selfsupervised_batch_predict.py',\n",
    "\n",
    "        arguments=[\n",
    "            sm_local_input_data,\n",
    "            sm_local_input_models,\n",
    "            sm_local_output,\n",
    "            \"--ensemble\", \"0\",\n",
    "            \"--tokenisor_data_dir\", sm_local_input_vocab,           \n",
    "            \"--filter\", str(usefilter),\n",
    "            \"--batch\", \"32\",\n",
    "            \"--filterstdthreshold\", str(filter_threshold_std),\n",
    "            \"--filepattern\",filepattern\n",
    "        ],\n",
    "\n",
    "        inputs=[\n",
    "                ProcessingInput(\n",
    "                    source=s3_input_data,\n",
    "                    s3_data_type = s3_data_type,\n",
    "                    destination=sm_local_input_data,\n",
    "                    s3_data_distribution_type=\"ShardedByS3Key\"),\n",
    "\n",
    "            ProcessingInput(\n",
    "                    source=s3_input_models,\n",
    "                    destination=sm_local_input_models,\n",
    "                    s3_data_distribution_type=\"FullyReplicated\"),\n",
    "            \n",
    "            ProcessingInput(\n",
    "                    source=s3_input_vocab,\n",
    "                    destination=sm_local_input_vocab,\n",
    "                    s3_data_distribution_type=\"FullyReplicated\")\n",
    "            ],\n",
    "\n",
    "\n",
    "        outputs=[ProcessingOutput(\n",
    "                source=sm_local_output, \n",
    "                destination=s3_output_predictions,\n",
    "                output_name='predictions')]\n",
    "    )\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}